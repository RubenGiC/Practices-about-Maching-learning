# Maching Learning Practices

# Practices 1 

**I have learned Linear Regression and Iterative Search for Optimal through several samples**
* Part 1
  * Iterative Search for Optimal with these functions
    
    <img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/Tex2Img_1616618057.jpg?raw=true" alt="f(x,y)">
    </br>
    <img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/Tex2Img_1616618271.jpg?raw=true" alt="Tex2Img_1616618271.jpg">

## Gradient Descent Implementation 

<p>The Gadient Descent algorithm is a general iterative optimization technique, where reach a local optimum.</p>

<img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/descarga.gif?raw=true" alt="Gradient Descent">

<p>For this I need a small constant that will be my learning rate, which is denoted by Î·. We must be careful with this constant, since if it is too large it may not reach the optimal local and if it is too small, it reaches the optimal local, but it entails a great computational cost. </p>
<p>Therefore, the optimal learning rate will depend on the topoligy of our loss landscape, which in turn depends on the data set.</p>

<img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/learning%20rate.png?raw=true" alt="learning rate.png">

# Bibliography
* https://www.jeremyjordan.me/nn-learning-rate/
* https://sigmoidal.ai/metodo-de-ensemble-vantagens-da-combinacao-de-diferentes-estimadores/

