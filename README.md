# Maching Learning Practices

# Practices 1 

**I have learned Linear Regression and Iterative Search for Optimal through several samples**
* Part 1
  * Iterative Search for Optimal
    * <img src="https://bit.ly/3fc5mX2" align="center" border="0" alt="E(u,v)=(u^{3}e^{v-2}-2v^{2}e^{-u})^{2}" width="243" height="21" />
    * <img src="https://bit.ly/3ciTCAf" align="center" border="0" alt="f(x,y)=(x+2)^{2}+2(y-2)^{2}+2sin(2 \pi x)sin(2 \pi y)" width="414" height="22" />

## Gradient Descent Implementation 

The Gadient Descent algorithm is a general iterative optimization technique, where reach a local optimum.
![Gradient Descent](https://github.com/RubenGiC/Practices-about-Maching-learning/)
