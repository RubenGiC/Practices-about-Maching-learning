# Maching Learning Practices

# Practices 1 

**I have learned Linear Regression and Iterative Search for Optimal through several samples**
* Part 1
  * Iterative Search for Optimal
    
    <img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/Tex2Img_1616618057.jpg?raw=true" alt="f(x,y)">
    </br>
    <img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/Tex2Img_1616618271.jpg?raw=true" alt="Tex2Img_1616618271.jpg">

## Gradient Descent Implementation 

<p>The Gadient Descent algorithm is a general iterative optimization technique, where reach a local optimum.</p>

<img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/descarga.gif?raw=true" alt="Gradient Descent">
