# Maching Learning Practices

# Practices 1 

**I have learned Linear Regression and Iterative Search for Optimal through several samples**
* Part 1
  * Iterative Search for Optimal


<img src="http://www.sciweavers.org/tex2img.php?eq=f(x,y)%3D(x%2B2)^{2}%2B2(y-2)^{2}%2B2sin(2%20\pi%20x)sin(2%20\pi%20y)&amp;bc=White&amp;fc=Black&amp;im=jpg&amp;fs=12&amp;ff=arev&amp;edit=0">

## Gradient Descent Implementation 

<p>The Gadient Descent algorithm is a general iterative optimization technique, where reach a local optimum.</p>

<img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/descarga.gif?raw=true" alt="Gradient Descent" id="img1" style="#img1{background-color: white;}">
