# Maching Learning Practices

# Practices 1 

**I have learned Linear Regression and Iterative Search for Optimal through several samples**
* Part 1
  * Iterative Search for Optimal

<img style="-webkit-user-select: none;margin: auto;background-color: hsl(0, 0%, 90%);transition: background-color 300ms;" src="https://camo.githubusercontent.com/70478826071f87235f10326e35b8d7709ccb004ba1dcdb70b7bef4fb79b97e57/68747470733a2f2f6269742e6c792f336663356d5832">

<img src="https://bit.ly/3ciTCAf" align="center" border="0" alt="f(x,y)=(x+2)^{2}+2(y-2)^{2}+2sin(2 \pi x)sin(2 \pi y)" width="414" height="22" />

## Gradient Descent Implementation 

<p>The Gadient Descent algorithm is a general iterative optimization technique, where reach a local optimum.</p>

<img src="http://github.com/RubenGiC/Practices-about-Maching-learning/blob/main/P1/Images/descarga.gif?raw=true" alt="Gradient Descent">
